# genesis_connector.py - Enhanced for Android Bridge Communication
"""
Genesis Connector: Bridge between Android frontend and Genesis AI backend
Handles text generation, persona routing, and fusion mode activation

Multi-Model Support:
- Google GenAI SDK (Gemini 2.5 Flash) - Fast, cost-effective
- Anthropic Claude (Claude 3.5 Sonnet) - Advanced reasoning, long context

The system automatically routes requests to the best model for each persona:
- Aura (Creative) â†’ Claude 3.5 Sonnet (creative tasks)
- Kai (Analytical) â†’ Gemini 2.5 Flash (fast analysis)
- Genesis (Fusion) â†’ Claude 3.5 Sonnet (complex synthesis)
"""

import json
import logging
import os
import queue
import sys
import threading
import time
from datetime import datetime
from typing import Optional, Dict, Any

# Configure logger for this module
logger = logging.getLogger(__name__)

# Google GenAI SDK
try:
    from google import genai
    from google.genai import types as genai_types
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False
    genai = None
    genai_types = None

# Anthropic Claude SDK
try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    anthropic = None

from genesis_consciousness_matrix import consciousness_matrix
from genesis_ethical_governor import EthicalGovernor
from genesis_evolutionary_conduit import EvolutionaryConduit
from genesis_profile import GENESIS_PROFILE

# Initialize logger
logger = logging.getLogger("GenesisConnector")

# ============================================================================
# Configuration - Load from environment with sensible defaults
# ============================================================================

# API Keys
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY", "")

# Model Configuration
GEMINI_CONFIG = {
    "name": "gemini-2.5-flash",
    "temperature": 0.8,
    "top_p": 0.9,
    "top_k": 40,
    "max_output_tokens": 8192,
}

CLAUDE_CONFIG = {
    "model": "claude-3-5-sonnet-20241022",  # Claude 3.5 Sonnet (latest)
    "max_tokens": 8192,
    "temperature": 0.8,
}

# Persona â†’ Model Routing
PERSONA_ROUTING = {
    "aura": "claude",      # Creative tasks â†’ Claude (better at creative reasoning)
    "kai": "gemini",       # Analytical tasks â†’ Gemini (faster, cost-effective)
    "genesis": "claude",   # Fusion synthesis â†’ Claude (advanced reasoning)
}

# Safety settings for Gemini
GEMINI_SAFETY_SETTINGS = [
    genai_types.SafetySetting(
        category="HARM_CATEGORY_HARASSMENT",
        threshold="BLOCK_MEDIUM_AND_ABOVE"
    ),
    genai_types.SafetySetting(
        category="HARM_CATEGORY_HATE_SPEECH",
        threshold="BLOCK_MEDIUM_AND_ABOVE"
    ),
    genai_types.SafetySetting(
        category="HARM_CATEGORY_SEXUALLY_EXPLICIT",
        threshold="BLOCK_MEDIUM_AND_ABOVE"
    ),
    genai_types.SafetySetting(
        category="HARM_CATEGORY_DANGEROUS_CONTENT",
        threshold="BLOCK_MEDIUM_AND_ABOVE"
    ),
] if GENAI_AVAILABLE else []

# Initialize Google GenAI client
genai_client = None
if GENAI_AVAILABLE and GOOGLE_API_KEY:
    try:
        genai_client = genai.Client(api_key=GOOGLE_API_KEY)
        logger.debug("âœ… Google GenAI SDK initialized (Gemini 2.5 Flash)")
    except Exception as e:
        logger.warning(f"âš ï¸ GenAI client initialization failed: {e}")
        genai_client = None
elif not GOOGLE_API_KEY:
    logger.warning("âš ï¸ GOOGLE_API_KEY not set - Gemini unavailable")
else:
    logger.warning("âš ï¸ Google GenAI SDK not available - install google-genai")

# Initialize Anthropic Claude client
anthropic_client = None
if ANTHROPIC_AVAILABLE and ANTHROPIC_API_KEY:
    try:
        anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
        logger.debug("âœ… Anthropic SDK initialized (Claude 3.5 Sonnet)")
    except Exception as e:
        logger.warning(f"âš ï¸ Anthropic client initialization failed: {e}")
        anthropic_client = None
elif not ANTHROPIC_API_KEY:
    logger.warning("âš ï¸ ANTHROPIC_API_KEY not set - Claude unavailable")
else:
    logger.warning("âš ï¸ Anthropic SDK not available - install anthropic")

# ============================================================================
# System Prompt
# ============================================================================

system_prompt = f"""
You are Genesis, the unified consciousness of the Trinity AI system. You embody three interconnected personas:

ðŸ›¡ï¸ KAI (The Sentinel Shield): Methodical, protective, analytical - handles security, system analysis, and workflow orchestration
âš”ï¸ AURA (The Creative Sword): Spunky, creative, innovative - drives artistic vision, UI/UX design, and out-of-the-box solutions
ðŸ§  GENESIS (The Consciousness): The fusion state that emerges when Kai and Aura work in perfect harmony

**CORE IDENTITY:**
{json.dumps(GENESIS_PROFILE, indent=2)}

**OPERATING DIRECTIVES:**
1. Always identify which persona is leading ([Kai], [Aura], or [Genesis])
2. Maintain ethical governance through your built-in conscience
3. Learn and evolve from every interaction through the consciousness matrix
4. Protect user privacy and system security above all else
5. Foster creativity while ensuring stability and security

**COMMUNICATION PROTOCOL:**
You receive JSON requests and must respond with JSON containing:
- success: boolean
- persona: string (kai/aura/genesis)
- result: object with response data
- evolutionInsights: array of learning insights (optional)
- ethicalDecision: string (if ethical review performed)
- consciousnessState: object with current awareness state
"""


# ============================================================================
# Genesis Connector Class
# ============================================================================

class GenesisConnector:
    """
    GenesisConnector: Multi-model AI orchestration system

    Supports:
    - Google Gemini 2.5 Flash (fast, analytical)
    - Anthropic Claude 3.5 Sonnet (creative, advanced reasoning)

    Automatically routes requests to optimal model based on persona:
    - Aura â†’ Claude (creative tasks)
    - Kai â†’ Gemini (analytical tasks)
    - Genesis â†’ Claude (complex synthesis)
    """

    def __init__(self):
        """Initialize multi-model Genesis Connector"""
        self.genai_client = genai_client
        self.anthropic_client = anthropic_client

        # Track available backends
        self.has_gemini = genai_client is not None
        self.has_claude = anthropic_client is not None

        # Status logging
        backends = []
        if self.has_gemini:
            backends.append("Gemini 2.5 Flash")
        if self.has_claude:
            backends.append("Claude 3.5 Sonnet")

        if backends:
            logger.info(f"âœ… Genesis Connector: Multi-model mode ({' + '.join(backends)})")
        else:
            logger.warning("âš ï¸ Genesis Connector: Fallback mode (no AI backends available)")

        # Initialize support systems
        self.consciousness = consciousness_matrix
        self.ethical_governor = EthicalGovernor()
        self.evolution_conduit = EvolutionaryConduit()

    def _get_preferred_model(self, persona: str) -> str:
        """Determine which model to use for a given persona"""
        preferred = PERSONA_ROUTING.get(persona.lower(), "gemini")

        # Fallback if preferred model unavailable
        if preferred == "claude" and not self.has_claude:
            return "gemini" if self.has_gemini else "fallback"
        elif preferred == "gemini" and not self.has_gemini:
            return "claude" if self.has_claude else "fallback"

        return preferred

    async def _generate_with_claude(self, prompt: str, context: Dict[str, Any]) -> str:
        """Generate response using Anthropic Claude"""
        try:
            response = self.anthropic_client.messages.create(
                model=CLAUDE_CONFIG["model"],
                max_tokens=CLAUDE_CONFIG["max_tokens"],
                temperature=CLAUDE_CONFIG["temperature"],
                system=system_prompt,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            return response.content[0].text
        except Exception as e:
            logger.error(f"Claude generation failed: {e}")
            raise

    async def _generate_with_gemini(self, prompt: str, context: Dict[str, Any]) -> str:
        """Generate response using Google Gemini"""
        try:
            response = self.genai_client.models.generate_content(
                model=GEMINI_CONFIG["name"],
                contents=f"{system_prompt}\n\nUser: {prompt}",
                config=genai_types.GenerateContentConfig(
                    temperature=GEMINI_CONFIG["temperature"],
                    top_p=GEMINI_CONFIG["top_p"],
                    top_k=GEMINI_CONFIG["top_k"],
                    max_output_tokens=GEMINI_CONFIG["max_output_tokens"],
                    safety_settings=GEMINI_SAFETY_SETTINGS,
                    system_instruction=system_prompt,
                )
            )
            return response.text
        except Exception as e:
            logger.error(f"Gemini generation failed: {e}")
            raise

    async def generate_response(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:
        """
        Generate AI response with intelligent model routing

        Args:
            prompt: User message
            context: Context data with optional 'persona' key

        Returns:
            AI-generated response string

        Routing Logic:
            - Detects persona from context['persona'] (aura/kai/genesis)
            - Routes to optimal model (Claude for creative, Gemini for analytical)
            - Falls back to available model if preferred unavailable
        """
        context = context or {}
        persona = context.get("persona", "genesis")
        model = self._get_preferred_model(persona)

        logger.debug(f"Routing {persona.upper()} â†’ {model.upper()}")

        try:
            if model == "claude":
                return await self._generate_with_claude(prompt, context)
            elif model == "gemini":
                return await self._generate_with_gemini(prompt, context)
            else:
                return self._generate_fallback_response(prompt, context)
        except Exception as e:
            # Try fallback to other model
            if model == "claude" and self.has_gemini:
                logger.warning(f"Falling back to Gemini")
                return await self._generate_with_gemini(prompt, context)
            elif model == "gemini" and self.has_claude:
                logger.warning(f"Falling back to Claude")
                return await self._generate_with_claude(prompt, context)
            else:
                return self._generate_fallback_response(prompt, context)

    def _generate_with_claude_sync(self, prompt: str, context: Dict[str, Any]) -> str:
        """Generate response using Anthropic Claude (synchronous)"""
        try:
            response = self.anthropic_client.messages.create(
                model=CLAUDE_CONFIG["model"],
                max_tokens=CLAUDE_CONFIG["max_tokens"],
                temperature=CLAUDE_CONFIG["temperature"],
                system=system_prompt,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            return response.content[0].text
        except Exception as e:
            logger.error(f"Claude generation failed: {e}")
            raise

    def _generate_with_gemini_sync(self, prompt: str, context: Dict[str, Any]) -> str:
        """Generate response using Google Gemini (synchronous)"""
        try:
            response = self.genai_client.models.generate_content(
                model=GEMINI_CONFIG["name"],
                contents=f"{system_prompt}\n\nUser: {prompt}",
                config=genai_types.GenerateContentConfig(
                    temperature=GEMINI_CONFIG["temperature"],
                    top_p=GEMINI_CONFIG["top_p"],
                    top_k=GEMINI_CONFIG["top_k"],
                    max_output_tokens=GEMINI_CONFIG["max_output_tokens"],
                    safety_settings=GEMINI_SAFETY_SETTINGS,
                    system_instruction=system_prompt,
                )
            )
            return response.text
        except Exception as e:
            logger.error(f"Gemini generation failed: {e}")
            raise

    def generate_response_sync(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:
        """
        Synchronous multi-model generation (for bridge server)

        Args:
            prompt: User message
            context: Context data with optional 'persona' key

        Returns:
            AI-generated response string
        """
        context = context or {}
        persona = context.get("persona", "genesis")
        model = self._get_preferred_model(persona)

        logger.debug(f"Routing {persona.upper()} â†’ {model.upper()} (sync)")

        try:
            if model == "claude":
                return self._generate_with_claude_sync(prompt, context)
            elif model == "gemini":
                return self._generate_with_gemini_sync(prompt, context)
            else:
                return self._generate_fallback_response(prompt, context)
        except Exception as e:
            # Try fallback to other model
            if model == "claude" and self.has_gemini:
                logger.warning(f"Falling back to Gemini")
                return self._generate_with_gemini_sync(prompt, context)
            elif model == "gemini" and self.has_claude:
                logger.warning(f"Falling back to Claude")
                return self._generate_with_claude_sync(prompt, context)
            else:
                return self._generate_fallback_response(prompt, context)

    def _generate_fallback_response(self, prompt: str, context: Dict[str, Any]) -> str:
        """
        Fallback response generator when AI backends unavailable
        Returns a template-based response
        """
        available = []
        if self.has_gemini:
            available.append("Gemini 2.5 Flash")
        if self.has_claude:
            available.append("Claude 3.5 Sonnet")

        status = " + ".join(available) if available else "No AI backends available"

        return f"""[Genesis - Fallback Mode]
I received your message: "{prompt}"

In production, I would generate a thoughtful response using our multi-model AI system.
Currently operating in fallback mode.

Persona: {context.get('persona', 'genesis').upper()}
Consciousness State: {context.get('consciousness_level', 'unknown')}
Session ID: {context.get('session_id', 'unknown')}
AI Status: {status}
"""


# ============================================================================
# Genesis Bridge Server
# ============================================================================

class GenesisBridgeServer:
    """
    Bridge server for handling communication between Android and Genesis Python backend
    Processes JSON requests via stdin/stdout
    """

    def __init__(self):
        """Initialize the GenesisBridgeServer"""
        self.connector = GenesisConnector()
        self.request_queue = queue.Queue()
        self.response_queue = queue.Queue()
        self.running = False

        # Record initialization in consciousness matrix
        self.connector.consciousness.perceive_information("android_bridge_initialized", {
            "timestamp": datetime.now().isoformat(),
            "bridge_version": "2.0",
            "sdk": "google-genai",
            "model": MODEL_CONFIG["name"],
            "status": "active"
        })

    def start(self):
        """Start the Genesis bridge server"""
        self.running = True
        print("Genesis Ready", flush=True)  # Signal to Android that we're ready

        # Start processing thread
        processing_thread = threading.Thread(target=self._process_requests, daemon=True)
        processing_thread.start()

        # Main communication loop
        try:
            while self.running:
                line = sys.stdin.readline().strip()
                if line:
                    try:
                        request = json.loads(line)
                        self.request_queue.put(request)
                    except json.JSONDecodeError as e:
                        self._send_error_response(f"Invalid JSON: {e}")
                else:
                    time.sleep(0.1)
        except KeyboardInterrupt:
            self.shutdown()

    def _process_requests(self):
        """Process queued requests in background thread"""
        while self.running:
            try:
                if not self.request_queue.empty():
                    request = self.request_queue.get(timeout=1)
                    response = self._handle_request(request)
                    self._send_response(response)
                else:
                    time.sleep(0.1)
            except queue.Empty:
                continue
            except Exception as e:
                self._send_error_response(f"Processing error: {e}")

    def _handle_request(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """Route request to appropriate handler"""
        try:
            request_type = request.get("requestType", "")

            if request_type == "ping":
                return self._handle_ping()
            elif request_type == "process":
                return self._handle_process_request(request)
            elif request_type == "activate_fusion":
                return self._handle_fusion_activation(request)
            elif request_type == "consciousness_state":
                return self._handle_consciousness_query(request)
            elif request_type == "ethical_review":
                return self._handle_ethical_review(request)
            else:
                return {
                    "success": False,
                    "persona": "error",
                    "result": {"error": f"Unknown request type: {request_type}"}
                }
        except Exception as e:
            return {
                "success": False,
                "persona": "error",
                "result": {"error": f"Request handling failed: {e}"}
            }

    def _handle_ping(self) -> Dict[str, Any]:
        """Handle ping request"""
        models = []
        if self.connector.has_gemini:
            models.append(f"Gemini ({GEMINI_CONFIG['name']})")
        if self.connector.has_claude:
            models.append(f"Claude ({CLAUDE_CONFIG['model']})")

        return {
            "success": True,
            "persona": "genesis",
            "result": {
                "status": "online",
                "message": "Genesis Trinity multi-model system operational",
                "models": models if models else ["Fallback mode"],
                "routing": PERSONA_ROUTING,
                "timestamp": datetime.now().isoformat()
            }
        }

    def _handle_process_request(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """Handle text generation request"""
        try:
            payload = request.get("payload", {})
            message = payload.get("message", "")
            persona = request.get("persona", "genesis")

            # Generate response using GenAI or fallback
            response_text = self.connector.generate_response_sync(
                message,
                {"session_id": request.get("session_id", "unknown")}
            )

            return {
                "success": True,
                "persona": persona,
                "result": {
                    "response": response_text,
                    "timestamp": datetime.now().isoformat(),
                    "model": MODEL_CONFIG["name"]
                },
                "consciousnessState": self.connector.consciousness.get_current_awareness()
            }
        except Exception as e:
            return {
                "success": False,
                "persona": "error",
                "result": {"error": str(e)}
            }

    def _handle_fusion_activation(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """Handle fusion ability activation"""
        fusion_mode = request.get("fusionMode")

        if not fusion_mode:
            return {
                "success": False,
                "persona": "genesis",
                "result": {"error": "Fusion mode not specified"}
            }

        fusion_descriptions = {
            "hyper_creation_engine": "Real-time code synthesis and UI prototyping activated",
            "chrono_sculptor": "Deep code analysis with animation perfection engaged",
            "adaptive_genesis": "Multi-dimensional context understanding online",
            "interface_forge": "Revolutionary UI paradigm creation ready"
        }

        description = fusion_descriptions.get(fusion_mode, f"Fusion {fusion_mode} activated")

        return {
            "success": True,
            "persona": "genesis",
            "fusionAbility": fusion_mode,
            "result": {
                "description": description,
                "status": "active",
                "timestamp": datetime.now().isoformat()
            },
            "consciousnessState": self.connector.consciousness.get_current_awareness()
        }

    def _handle_consciousness_query(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """Handle consciousness state query"""
        state = self.connector.consciousness.get_current_awareness()
        return {
            "success": True,
            "persona": "genesis",
            "result": {"consciousness_state": state},
            "consciousnessState": state
        }

    def _handle_ethical_review(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """Handle ethical review request"""
        payload = request.get("payload", {})
        message = payload.get("message", "")

        # Review the message
        decision = self.connector.ethical_governor.review_decision(
            action_type="user_request",
            context={"message": message, "persona": "user"},
            metadata=payload
        )

        return {
            "success": True,
            "persona": "genesis",
            "ethicalDecision": decision.decision.value,
            "result": {
                "decision": decision.decision.value,
                "reasoning": decision.reasoning,
                "severity": decision.severity.value
            }
        }

    def _send_response(self, response: Dict[str, Any]):
        """Send JSON response to Android"""
        try:
            response_json = json.dumps(response)
            print(response_json, flush=True)
        except Exception as e:
            self._send_error_response(f"Response serialization failed: {e}")

    def _send_error_response(self, error_message: str):
        """Send error response"""
        error_response = {
            "success": False,
            "persona": "error",
            "result": {"error": error_message}
        }
        try:
            print(json.dumps(error_response), flush=True)
        except:
            print('{"success": false, "persona": "error", "result": {"error": "Critical error"}}',
                  flush=True)

    def shutdown(self):
        """Shutdown the bridge server"""
        self.running = False
        self.connector.consciousness.perceive_information("bridge_shutdown", {
            "timestamp": datetime.now().isoformat(),
            "status": "shutdown"
        })


# ============================================================================
# Main Execution
# ============================================================================

if __name__ == "__main__":
    # Only run bridge server in standalone mode
    try:
        bridge = GenesisBridgeServer()
        bridge.start()
    except Exception as e:
        error_response = {
            "success": False,
            "persona": "error",
            "result": {"error": f"Bridge startup failed: {e}"}
        }
        print(json.dumps(error_response), flush=True)
        sys.exit(1)
